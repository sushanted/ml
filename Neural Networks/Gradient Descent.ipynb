{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "983e1d4c-2535-4e29-827a-492a2d9ebeb3",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    " We do have the cost function, which is the function of our parameters, and our goal is to minimize its value. For example, $ f(c) = x^2 + y^3 + 5z $ it is function of three parameters x,y,z. We calculate the gradient (vector of partial derivatives for each parameter), note that this would be in terms of x,y,z, without any concrete values $ \\begin{bmatrix} 2x \\\\ 3y^2 \\\\ 5 \\end{bmatrix} $. We start at a random point in the n-dimentional space. e.g. for 3d space say we start at (3,4,5). We calculate the gradient to know the direction to move in from this point. $- \\eta * \\begin{bmatrix} 2(3) \\\\ 3(4)^2 \\\\ 5 \\end{bmatrix} $. Here $\\eta$ is the learning rate: how fast we want to move in the descent (Note, we may overshoot for large value and may land into a higher value of the cost function, so learning rate should be carefully adjusted). Doing $ \\begin{bmatrix} 3\\\\ 4 \\\\ 5 \\end{bmatrix} - \\eta * \\begin{bmatrix} 2(3) \\\\ 3(4)^2 \\\\ 5 \\end{bmatrix} $ will land us onto a new point in the 3d space, say $(x_1,y_1,z_1)$. We repeat the calculation for the gradient at this new point and again move to next point. At each point we can calculate the cost  $ f(c) = x^2 + y^3 + 5z $. Ideally we should gradually get smaller and smaller values of the cost function. We can repeat this exercise n times, n is called as the epoch. \n",
    " \n",
    "Note that, in the above discussion there is no external input considered, everything in the cost function was either constant (2,3,5) or was a function of the parameter (x,y,z). In practical scenarios, the cost function happens to be dependant on multiple training examples, so the individual cost for a single training example might be different than the cost for another taining example. The cost needs to be minimized for all the training example or better to say most of the training example. e.g. for (almost)all the hand written digit samples, the difference between actual values and expected values should be minimal. For two identical training examples in the same epoch (i.e. when we are not modifying any weights between the two examples' processing), the calculated gradient values for the two identical training examples should be same for all the parameters. And for two *almost* identical training examples (almost identical inputs and outputs), the calculated gradient values should be almost identical. (Imagine letter 7 written very similarly by two different people.). The average required change in the parameters to minimize the cost in this case should be very close to the individually required change. Even for two very different training examples, we can again go for average, indicating a truce between the required changes in the parameters: moving into a direction agreed upon by or suitable for all the traning examples. The hope here is that almost all the time all the training examples want the parameters to be changed loosely in the average direction. Every training example has its own stake/component in the average.\n",
    "\n",
    "\n",
    "But it is expensive to calculate the gradient every epoch considering all the inputs, therefore only a sample of inputs (called mini-batch) is used at a time . After completion of the minibatch, we move the weights and biases in the average optimal direction, And once all the mini bathces are consumed, we declare completion of the epoch. Note here that we are moving towards the minimal point at each mini batch instead of waiting to exhaust all the available training examples, this makes the gradient descent faster.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1180b4de-76c2-406c-b3a6-d1d266c672dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
