## Gradient Descent

 We do have the cost function, which is the function of our parameters, and our goal is to minimize its value. For example, $ f(c) = x^2 + y^3 + 5z $ it is function of three parameters x,y,z. We calculate the gradient (vector of partial derivatives for each parameter), note that this would be in terms of x,y,z, without any concrete values $ \begin{bmatrix} 2x \\ 3y^2 \\ 5z \end{bmatrix} $. We start at a random point in the n-dimentional space. e.g. for 3d space say we start at (3,4,5). We calculate the gradient to know the direction to move in at this point. $\eta * \begin{bmatrix} 2(3) \\ 3(4)^2 \\ 5(5) \end{bmatrix} $. Here $\eta$ is the learning rate: how fast we want to move in the descent (Note, we may overshoot for large value and may land into a higher value of the cost function, so learning rate should be carefully adjusted). Doing $ \begin{bmatrix} 3\\ 4 \\ 5 \end{bmatrix} - \eta * \begin{bmatrix} 2(3) \\ 3(4)^2 \\ 5(5) \end{bmatrix} $ will land us onto a new point in the 3d space, say $(x_1,y_1,z_1)$. We repeat the calculation for the gradient at this new point and again move to next point. At each point we can calculate the cost  $ f(c) = x^2 + y^3 + 5z $. Ideally we should gradually get smaller and smaller values of the cost function. We can repeat this exercise n times, n is called as the epoch.   
 
Generally the cost funtion depends on the parameters as well as multiple inputs, the cost needs to be minimized for all the input samples or better to say most of the inputs. e.g. for (almost)all the hand written digit samples, the difference between actual values and expected values should be minimal. But it is expensive to calculate the gradient every epoch considering all the inputs, therefore only a sample of inputs (called mini-batch) is used at a time and once all the input is consumed, we declare completion of the epoch. Note here that we are moving towards the minimal point at each mini batch instead of waiting to complete a complete epoch, this makes the gradient descent faster.