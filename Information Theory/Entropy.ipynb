{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330034c5-8bc8-487b-a97a-9df29c9f8474",
   "metadata": {},
   "source": [
    "# Entropy "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80530cf3-3c5b-4ed2-98cc-063b198739c0",
   "metadata": {},
   "source": [
    "Very good article : https://arxiv.org/pdf/1405.2061.pdfb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04dd69d8-d2c6-467d-afd4-73be3bdc1227",
   "metadata": {},
   "source": [
    "Consider a system where there are 5 symbols, A,B,C,D,E having probabilities of occurences as follows.\n",
    "\n",
    "$ A : \\frac{1}{16} $  \n",
    "$ B : \\frac{1}{16} $  \n",
    "$ C : \\frac{2}{16} $  \n",
    "$ D : \\frac{4}{16} $  \n",
    "$ E : \\frac{8}{16} $  \n",
    "\n",
    "Note the numbers in above probabilites are purposefully taken to be power of 2.  \n",
    "\n",
    "We can create a binary code with 3 bits to represent all these symbols.  \n",
    "\n",
    "A : 000  \n",
    "B : 001  \n",
    "C : 010  \n",
    "D : 011  \n",
    "E : 100  \n",
    "\n",
    "And finally construct a message BEEDEEDEECEEDACD as 001.100.100.011.100.100.011.100.100.010.100.100.011.000.010.011 which requires 16 * 3 = 48 bits in total.   \n",
    "\n",
    "16 symbols in 48 bits, $\\frac{48}{16} = 3$ bits per symbol (which is obvious because of fixed length encoding).    \n",
    "\n",
    "Note that the message follows the frequency of the characters in the system (8 Es, 4 Ds, 2 Cs and 1 A,B).  \n",
    "\n",
    "Instead of this, be can utilize the knowledge of the frequency/probability for each symbol in the system and can create a variable length code (Huffman encoding), which can be decoded as well due to no suffix sharing (unique suffixes: 0,1,11,111).  \n",
    "\n",
    "\n",
    "A : 1111  \n",
    "B : 1110  \n",
    "C : 110  \n",
    "D : 10  \n",
    "E : 0\n",
    "\n",
    "The same message now can be constructed as 1110.0.0.10.0.0.10.0.0.100.0.0.10.1111.110.10 which requires only 8 * 1 + 4 * 2 + 2 * 3 + 4 * 1 + 4 * 1 = 30 bits.  \n",
    "\n",
    "16 symbols in 30 bits, $\\frac{30}{16} = 1.875$ bits per symbol.  \n",
    "\n",
    "In general we can observe a similar compression in all the messages in the system which follow the same probability distribution for symbols A,B,C,D and E.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8ece1c-89bc-4d40-8417-7adfc3203cb1",
   "metadata": {},
   "source": [
    "#### How did we create the above encoding?\n",
    "\n",
    "\n",
    "We are trying to minimize the average size of the messages.  \n",
    "\n",
    "High probability symbols should be encoded with minimal bits so the average message size (which contains high number of high probability of symbols) would be minimal.  \n",
    "\n",
    "Low probability symbols can be encoded with the remaining number of symbols, due to low probability those symbols will rarely occur in any message and thus even if they have more number of bits, the total contribution to the complete message still would be low.  \n",
    "\n",
    "In general to represent N symbols we need $log_2(N)$ bits, as b bits can represent $N = 2^b$ symbols. \n",
    "\n",
    "E has probability $\\frac{8}{16}$ which means, half of the times (8/16) the symbol is E. With a single binary bit, we can tell if the symbol is E or any other symbol, therefore one bit is sufficient (and efficient due to high probability) to encode presence of E. Or in other words, 16 can be divided into two groups of size 8: one which contains all Es and other containing the other symbols and $log_2(\\frac{16}{8})=log_2(2) = 1$ bit is sufficient (and again efficient) to represent the two groups: 0-> E, 1 ->{A,B,C,D}     \n",
    "\n",
    "D has probability $\\frac{4}{16}$ which mean, $\\frac{1}{4}$ th times the symbol is D. In the 1s group, probabilistically there would be 4 Ds and 4 other symbols. We can further use one more bit to do the binary partition. 10-> D, 11-> {A,B,C}  \n",
    "\n",
    "\n",
    "\n",
    "C has probability $\\frac{2}{16}$ which mean, $\\frac{1}{8}$ th times the symbol is C. In the 11s group, probabilistically there would be 2 Cs and 2 other symbols. We can further use one more bit to do the binary partition. 110-> C, 111-> {A,B}  \n",
    "\n",
    "We can divide the total 16 possibilities into 8 groups, among which C can be represented as one group (having all C's in the group) and remaining symbols can be represented by remaining 14 possibilities. $log_2(\\frac{16}{2})=log_2(8) = 3$ bits are sufficient (and again efficient) to represent the group of all Cs.     \n",
    "\n",
    "A,B have probability $\\frac{1}{16}$ which mean, $\\frac{1}{16}$ th times the symbol is C. We can divide the total 16 possibilities into 16 groups of size 1, among which A,B can be represented as one group each (having A or B in the group) $log_2(\\frac{16}{1})=log_2(16) = 4$ bits are sufficient to represent A and B.  \n",
    "\n",
    "![huffman_coding](Huffman_coding.png)  \n",
    "\n",
    "Source: https://www.geogebra.org/graphing/nypxv7sq  \n",
    "\n",
    "\n",
    "For a message in this system which follows the probability distribution mentioned before, follows the exptected no. of bits per symbol in the message:  \n",
    "E(bits per message) = $\\sum_i p_i b_i$   \n",
    "\n",
    "Where $p_i$ is the probability of a symbol and $b_i$ is the number of bits required to represent the symbol.  \n",
    "\n",
    "For our system:  \n",
    "\n",
    "$\\sum_i p_i b_i$ = $ \\frac{8}{16}log_2(\\frac{16}{8}) + \\frac{4}{16}log_2(\\frac{16}{4}) + \\frac{2}{16}log_2(\\frac{16}{2}) + \\frac{1}{16}log_2(\\frac{16}{1}) + \\frac{1}{16}log_2(\\frac{16}{1}) $  \n",
    "$= 0.5 * 1 + 0.5 + 0.375 + 0.25 + 0.25 $  \n",
    "$ = 1.875$  \n",
    "\n",
    "Note: this is exactly same as the calculation for a sample message above.  \n",
    "\n",
    "We can correlated b with p, as we observed in our bit calculations, $b = log_2(\\frac{1}{p})$  \n",
    "\n",
    "Number of bits required to represent a symbol is log of receiprocal of its probability.  \n",
    "\n",
    "So we can improvise our formula as:  \n",
    "\n",
    "\n",
    "E(bits per message) = $\\sum_i p_i log_2(\\frac{1}{p_i}) = -\\sum_i p_i log_2(p_i)$   \n",
    "\n",
    "This is the exact Shannon's entropy formula.  \n",
    "\n",
    "Note: In the cases where the probabilities (nominator and denominator) are power of 2, the encoding's bits per message converges to the entropy (As in our case). For non power-2 probabilities, due to possible inefficient representation in integral number of bits, the number of bits per message don't converge to the entropy value. There are encodings like arithmetic encoding (https://en.wikipedia.org/wiki/Arithmetic_coding) where the complete messages can be represented in the form of a string of bits. An example : https://commons.wikimedia.org/wiki/File:Arithmetic_coding_visualisation_circle.svg#/media/File:Arithmetic_coding_visualisation_circle.svg  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3544e9-831d-450a-8548-73c2aa1db738",
   "metadata": {},
   "source": [
    "#### Research : Why entropy is the lower bound on the required storage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0f1b87-1897-40f7-9f37-b6c396f1c344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
